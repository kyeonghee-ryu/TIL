# Linear Regression

### 1. 정의

-데이터를 가장 잘 설명하는 식 y=ax+b 찾기 (최적의 a,b 찾기)

### 2. MSE

-error: 실제값과 회귀선 (y = a hat + b hat)값의 차이

-error를 최소화하는 선형회귀식을 찾는 것이 목적

-최소화 방법은? MSE(mean square error)

-MSE: 오차의 제곱을 평균 낸 값

-MES가 가장 작은 값을 가질 때, a,b는 최적의 값을 가짐



### 3. Linear Regression Model

H(x) = Wx + b 

H(x) :**가설(Hypothesis)** 

-**W( Weight)**, **b(bias)** 의 값에 따라 선의 모양이 달라짐

-H(x) = Wx + b 이 각 데이터의 분포와의 차이를 계산하여 가장 적은 것이 이 모델에 적합한 선이다

-**비용함수(Cost Function)** 라고 부른다.

이 비용함수를 통해 우리가 세운 가설과 실제 나타내는 값이 얼마나 다른가를 가늠해 볼 수 있다.



위의 간단한 식을 빌려서 이야기 해보자면 실제로 위의 식으로는 값이 얼마나 다른지 구할 수 없다. 위와 같은 형태는 차를 계산할 뿐 가설 데이터보다 더 큰 실제의 값을 나타내는 것이 주어졌을 때 표현할 수가 없기 때문이다.

이를 나타내기 위해 해당 식에 제곱을 취해주게 된다. 또한 이를 바탕으로 실제로 더 차이가 큰 값에는 더 큰 패널티를 줄 수 있도록 이러한 가중치를 자동적으로 부여할 수 있게 된다. 비용함수에 대한 식을 나타내면 아래와 같다.



![img](https://t1.daumcdn.net/cfile/tistory/2178CA335914A02B01)



일반적으로 m을 데이터의 개수라 하면 각각의 우리가 **예측한 값 X**와 실제 **데이터 값 Y**의 차이를 제곱한 값의 전체 데이터 개수만큼의 결과 값을 더하여 다시 데이터의 개수만큼 크기로 나누어 평균을 구하게 되는 것이다.



결과적으로, 이 **회귀 분석(Linear Regression)**을 통하여 그래프 상에서 선을 그렸을 때 **W(가중치)** 값과 미세하게 조절되는 **b(바이어스)**의 값을 찾아 가설과 실제 데이터의 차가 가장 작은 은 최소값을 찾는 것이 머신러닝에서의 선형 회귀(Linear Regression)의 학습이라고 볼 수있다.

